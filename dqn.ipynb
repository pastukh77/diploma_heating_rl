{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from env.env import RoomsEnv, bms_data\n",
    "env = RoomsEnv(\"2022-12-25 06:00:00\", 20, bms_data)\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.n\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAV80lEQVR4nO3dfbRldX3f8feHh8iT4SGMiAw4tjEqkgh2QiBkGWpKBHygLXEB1Robm6ldNhJjEgW7EunKWiZNatTWRtFYUCmmETUGiUpQaqwGHQSRB0moYoVAGFZEQCPCvd/+sfcdLpM7956ZOfueO/v3fq111jlnn4f9vfs753v2fM9v/3aqCknS+Owx6wAkScOwwEvSSFngJWmkLPCSNFIWeEkaKQu8JI2UBV6aoiRXJ/m3s45DAgu8Ri7J7Un+PsmDSe5OclGSA1Zp3S9P8tnVWJe0FAu8WvDCqjoAOBY4DjhvtuFIq8MCr2ZU1d3AJ+gKPUlOSPK5JPcl+XKSkxee2+99fy3JA0m+nuQl/fI3Jnn/oudtSFJJ9lq8riTPAN4BnNj/7+G+of8+aVsWeDUjyXrgNOC2JEcAHwN+CzgE+FXgsiTrkuwPvA04raoeD/wkcP2OrKuqbgFeCXy+qg6oqoOm9odIE7LAqwUfSfIA8E3gHuA3gZcCV1TVFVU1X1VXApuB0/vXzAPHJNm3qu6qqptmErm0CyzwasE/7/fETwaeDhwKPBl4cd+eua9vofwUcHhVfQc4i24P/K4kH0vy9NmELu08C7yaUVX/G7gI+D26vfn3VdVBiy77V9Vv98/9RFWdAhwOfBV4V/823wH2W/S2T1xuldP+G6QdYYFXa94CnAJ8Dnhhkucl2TPJPklOTrI+yWFJzuh78Q8BD9K1bKDrxT8nyVFJDmT5ETl/C6xP8gOD/TXSMizwakpVbQHeC7waOAM4H9hCt0f/a3SfiT2AXwH+Bvg74KeBf9+//krgj4AbgGuBy5dZ3aeAm4C7k9w7wJ8jLSue8EOSxsk9eEkaKQu8JI2UBV6SRsoCL0kjtdfKT1k9hx56aG3YsGHWYUjSbuPaa6+9t6rWLfXYmirwGzZsYPPmzbMOQ5J2G0m+sb3HbNFI0khZ4CVppCzwkjRSFnhJGikLvCSN1KCjaJLcDjwAzAGPVNXGIdcnSXrUagyT/KdV5Ux6krTK1tQ4+J11wZ/exM1/c/+sw5DWtKd+/xaOfeiLsw5DSzjwwIM45RffNPX3HbrAF/DJJAW8s6ou3PYJSTYBmwCOOuqogcOR2vXiB97Hs77/JebJrEPRNr7zvUOA6Rf4QeeDT3JEVd2Z5AnAlcAvVdVntvf8jRs3lkeySgO56AVQ8/Bvrph1JJqiJNdu7/fNQUfRVNWd/fU9wIeB44dcn6RlzM9BHDjXksGynWT/JI9fuA38LHDjUOuTtIKat8A3Zsge/GHAh5MsrOd/VtXHB1yfpOWUe/CtGazAV9XXgGcN9f6SdtD8HOyx56yj0Cry61xqRc1DLPAtscBLrbBF0xyzLbVift4WTWMs8FIrHEXTHLMttcIWTXPMttQKR9E0xwIvtcJRNM2xwEutsEXTHLMttaLKFk1jLPBSK+bnIE4V3BILvNSKmrMH3xgLvNSK8kCn1ljgpVY4H3xzzLbUCls0zbHAS61wFE1zLPBSK2zRNMdsS63wQKfmmG2pFY6iaY4FXmqFLZrmmG2pFY6iaY4FXmpBVXdti6YpFnipBfNz3bUtmqaYbakFNd9dW+CbYralFpR78C0y21ILFlo09uCbYoGXWrC1RWOBb4kFXmqBLZommW2pBfP9HrwtmqZY4KUWOIqmSWZbaoEtmiaZbakFjqJp0uAFPsmeSa5LcvnQ65K0HY6iadJq7MGfC9yyCuuRtD22aJo0aLaTrAeeD7x7yPVIWoEtmiYN/XX+FuDXgfntPSHJpiSbk2zesmXLwOFIjVqYTdI9+KYMlu0kLwDuqaprl3teVV1YVRurauO6deuGCkdqmy2aJg2Z7ZOAFyW5HfgA8Nwk7x9wfZK2pzzQqUWDFfiqOq+q1lfVBuBs4FNV9dKh1idpGc4H3ySzLbVga4vGPfiW7LUaK6mqq4GrV2NdkpZgi6ZJ7sFLLZh3LpoWmW2pBbZommSBl1qwtUXjR74lZltqgaNommS2pRbYommSBV5qgaNommSBl1pgi6ZJZltqwdbJxtyDb4kFXmqBk401yWxLLdg6H7wf+ZaYbakFnrKvSRZ4qQW2aJpktqUWeMq+JlngpRbYommSBV5qQTmbZIvMttQCR9E0yWxLLbBF0yQLvNQCR9E0yWxLLXAUTZMs8FILbNE0yQIvtcBRNE0y21ILnA++SRZ4qQVb54PPbOPQqrLASy3wlH1NssBLLbBF0yQLvNQCT9nXJLMttcAWTZMs8FILFs7JaoumKRZ4qQW2aJpktqUW1BwQh0k2ZrACn2SfJF9I8uUkNyW5YKh1SVpBzdueadBeA773Q8Bzq+rBJHsDn03yZ1X1lwOuU9JS5udszzRosAJfVQU82N/du7/UUOuTtIyadwRNgwb9Sk+yZ5LrgXuAK6vqmiWesynJ5iSbt2zZMmQ4Urtq3j34Bg2a8aqaq6pjgfXA8UmOWeI5F1bVxqrauG7duiHDkdo1P2cPvkETtWiSrAN+Ediw+DVV9QuTvL6q7kvyaeBU4MYdD1PSLnEPvkmT9uD/BPgL4M+BuUle0H8pPNwX932BU4Df2akoJe2a8kfWFk1a4Perqtft4HsfDlycZE+6VtD/qqrLd/A9JE2DLZomTVrgL09yelVdMekbV9UNwHE7F5akqXIUTZMm/T/buXRF/ntJHugv9w8ZmKQpskXTpIn24Kvq8UMHImlA8x7J2qKJD3RK8iLgOf3dq+2nS7sRR9E0aaKMJ/ltujbNzf3l3CRvGjIwSVNki6ZJk+7Bnw4cW9Wd9yvJxcB1wHlDBSZpihxF06Qd+Uo/aNHtA6cch6QhOYqmSZPuwb8JuK4/GjV0vfjXDxaVpOmyRdOkSUfRXJrkauDH+0Wvq6q7B4tK0nRV2aJp0LJf6Ume3l8/m+7I1Dv6y5P6ZZJ2B/Nzns2pQSvtwf8KsAn4L0s8VsBzpx6RpOmrOXvwDVq2wFfVpv7maVX1vcWPJdlnsKgkTZen7GvSpL+6fG7CZZLWIk/Z16Rl9+CTPBE4Atg3yXF0I2gAfhDYb+DYJE2LLZomrdSDfx7wcrozMr150fIHgPMHiknStDmKpkkr9eAvppvT/cyqumyVYpI0bR7J2qRJx8FfluT5wDOBfRYt/09DBSZpimoOsveso9Aqm3SysXcAZwG/RNeHfzHw5AHjkjRNjqJp0qQ/q/9kVb0M+FZVXQCcCPzIcGFJmipH0TRp0owvjIH/bpInAQ/THdkqaXfgZGNNmnSysT9NchDwu8CX6I5ifddQQUmaMicba9KKBT7JHsBVVXUfcFmSy4F9qurbQwcnaUo8ZV+TVvxK70/y8fZF9x+yuEu7GU/Z16RJM35VkjMTp6OTdku2aJo0acb/HfDHwENJ7k/yQJL7B4xL0jR5oFOTJj3Q6fFDByJpQI6iadJEBT7Jc5ZaXlWfmW44kgZhi6ZJkw6T/LVFt/cBjgeuxRN+SLsHR9E0adIWzQsX309yJPCWIQKSNABH0TRpZzN+B/CMaQYiaUC2aJo0aQ/+v9IdvQrdl8KxdEe0StodOIqmSZP24Dcvuv0IcGlV/Z8B4pE0BEfRNGnSHvzFSdb1t7dM8pq+T/9e4DC6vf8Lq+qtOxuopF1gi6ZJy2Y8nTcmuRe4FfirJFuS/MYE7/0I8NqqOho4AXhVkqN3PWRJO8xT9jVppa/01wAnAT9eVYdU1cHATwAnJXnNci+sqruq6kv97QeAW+hO4C1ptTkffJNWyvi/Bs6pqq8vLKiqrwEvBV426UqSbACOA65Z4rFNSTYn2bxly0TdH0k7yhZNk1bK+N5Vde+2C/s+/EQneExyAHAZ8MtV9Q/mr6mqC6tqY1VtXLdu3SRvKWlHecq+Jq1U4L+/k48BkGRvuuJ+SVV9aEcCkzRFtmiatNIommdtZ9bI0E1ZsF391MJ/CNxSVW/eyfgkTUPNOUyyQcsW+KralX8RJ9H18L+S5Pp+2flVdcUuvKekHVX9MYq2aJoz6YFOO6yqPku3py9plubnumtbNM0x49LYlQW+VWZcGrua765t0TTHAi+NnS2aZplxaey2tmjcg2+NBV4aO1s0zbLAS2M33xd4WzTNMePS2JUFvlVmXBo7h0k2y4xLY7cwisYefHMs8NLYbW3RWOBbY4GXxs4WTbPMuDR2tmiaZYGXxm5hNklbNM2xwEtjt7VF4+SurbHAS2Nni6ZZFnhp7BxF0ywLvDR2jqJplhmXxs7JxpplgZfGzsnGmmXGpbFzPvhmWeClsdvaovHj3hozLo2dp+xrlhmXxs4WTbMs8NLYOYqmWRZ4aexs0TTLjEtj55GszbLAS2PnKJpmmXFp7GzRNMuMS2PnKJpmWeClsXMUTbMGK/BJ3pPkniQ3DrUOSROwRdOsITN+EXDqgO8vaRKOomnWYAW+qj4D/N1Q7y9pQuVskq2aecaTbEqyOcnmLVu2zDocaXy2nrJv5h93rbKZZ7yqLqyqjVW1cd26dbMORxofWzTNmnmBlzQwT9nXLDMujd3WFo178K0ZcpjkpcDngacluSPJK4Zal6Rl2KJp1l5DvXFVnTPUe0vaAY6iaZYZl8bOFk2zLPDS2LkH3ywzLo2do2iaZcalsXOysWZZ4KWxc7KxZplxaeycD75ZFnhp7Kq6a1s0zbHAS2Nni6ZZZlwau5oDAsmsI9Eqs8BLY1fztmcaZYGXxm5+zvZMo8y6NHY15wiaRlngpbGrskXTKAu8NHa2aJpl1qWxKwt8q8y6NHaOommWBV4aO1s0zTLr0tjVvKNoGmWBl8bOHnyzzLo0dvP24FtlgZfGrubdg2+UWZfGzhZNs8y6NHbzc7ZoGmWBl8bOUTTNssBLY2eLpllmXRo7R9E0ywIvjZ2jaJpl1qWxs0XTLLMujZ2TjTXLAi+NnZONNcusS2PnKfuaNWiBT3JqkluT3Jbk9UOuS9J2eMq+Zg1W4JPsCbwdOA04GjgnydFDrU/SdtiiadZeA7738cBtVfU1gCQfAM4Abp76mt750/DI96b+ttIofOsbcOTxs45CMzBkgT8C+Oai+3cAP7Htk5JsAjYBHHXUUTu3pkN/BOYe2rnXSmO37mlwzJmzjkIzMGSBn0hVXQhcCLBx48baqTc5813TDEmSRmHIxtydwJGL7q/vl0mSVsGQBf6LwFOTPCXJDwBnAx8dcH2SpEUGa9FU1SNJ/gPwCWBP4D1VddNQ65MkPdagPfiqugK4Ysh1SJKW5uBYSRopC7wkjZQFXpJGygIvSSOVqp07tmgISbYA39jJlx8K3DvFcIZgjLturccHxjgtxjiZJ1fVuqUeWFMFflck2VxVG2cdx3KMcdet9fjAGKfFGHedLRpJGikLvCSN1JgK/IWzDmACxrjr1np8YIzTYoy7aDQ9eEnSY41pD16StIgFXpJGarcv8GvxxN5Jjkzy6SQ3J7kpybn98kOSXJnkr/vrg9dArHsmuS7J5f39pyS5pt+ef9RP9TzL+A5K8sEkX01yS5IT19p2TPKaPs83Jrk0yT6z3o5J3pPkniQ3Llq25HZL5219rDckefYMY/zdPtc3JPlwkoMWPXZeH+OtSZ43i/gWPfbaJJXk0P7+TLbhSnbrAr+GT+z9CPDaqjoaOAF4VR/X64GrquqpwFX9/Vk7F7hl0f3fAX6/qn4Y+BbwiplE9ai3Ah+vqqcDz6KLdc1sxyRHAK8GNlbVMXRTY5/N7LfjRcCp2yzb3nY7DXhqf9kE/MEMY7wSOKaqfgz4K+A8gP7zczbwzP41/73//K92fCQ5EvhZ4P8tWjyrbbi8qtptL8CJwCcW3T8POG/WcS0R558ApwC3Aof3yw4Hbp1xXOvpPujPBS4HQndU3l5Lbd8ZxHcg8HX6wQCLlq+Z7cij5x4+hG767cuB562F7QhsAG5cabsB7wTOWep5qx3jNo/9C+CS/vZjPtt055k4cRbxAR+k29m4HTh01ttwuctuvQfP0if2PmJGsSwpyQbgOOAa4LCquqt/6G7gsFnF1XsL8OvAfH//h4D7quqR/v6st+dTgC3A/+jbSO9Osj9raDtW1Z3A79Htzd0FfBu4lrW1HRdsb7ut1c/RLwB/1t9eEzEmOQO4s6q+vM1DayK+be3uBX5NS3IAcBnwy1V1/+LHqvuan9kY1SQvAO6pqmtnFcME9gKeDfxBVR0HfIdt2jFrYDseDJxB92X0JGB/lvhv/Voz6+22kiRvoGt1XjLrWBYk2Q84H/iNWccyqd29wK/ZE3sn2ZuuuF9SVR/qF/9tksP7xw8H7plVfMBJwIuS3A58gK5N81bgoCQLZ/qa9fa8A7ijqq7p73+QruCvpe34z4CvV9WWqnoY+BDdtl1L23HB9rbbmvocJXk58ALgJf0XEayNGP8x3Rf5l/vPzXrgS0meuEbi+wd29wK/Jk/snSTAHwK3VNWbFz30UeDn+9s/T9ebn4mqOq+q1lfVBrrt9qmqegnwaeDn+qfNOsa7gW8meVq/6GeAm1lD25GuNXNCkv36vC/EuGa24yLb224fBV7WjwQ5Afj2olbOqkpyKl3b8EVV9d1FD30UODvJ45I8he7HzC+sZmxV9ZWqekJVbeg/N3cAz+7/na6ZbfgYs/4RYAo/gpxO92v7/wXeMOt4+ph+iu6/vzcA1/eX0+l63FcBfw38OXDIrGPt4z0ZuLy//Y/oPji3AX8MPG7GsR0LbO635UeAg9fadgQuAL4K3Ai8D3jcrLcjcCndbwIP0xWiV2xvu9H9uP72/jP0FboRQbOK8Ta6XvbC5+Ydi57/hj7GW4HTZhHfNo/fzqM/ss5kG650caoCSRqp3b1FI0naDgu8JI2UBV6SRsoCL0kjZYGXpJGywGu0kswluX7RZdlJyZK8MsnLprDe2xdmGZRmyWGSGq0kD1bVATNY7+1046DvXe11S4u5B6/m9HvY/znJV5J8IckP98vfmORX+9uvTjef/w1JPtAvOyTJR/plf5nkx/rlP5Tkk+nmhH833UEvC+t6ab+O65O8cxWmuJW2ssBrzPbdpkVz1qLHvl1VPwr8N7pZNbf1euC46uYlf2W/7ALgun7Z+cB7++W/CXy2qp4JfBg4CiDJM4CzgJOq6lhgDnjJNP9AaTl7rfwUabf1931hXcqli65/f4nHbwAuSfIRuikSoJuC4kyAqvpUv+f+g8BzgH/ZL/9Ykm/1z/8Z4J8AX+ymqWFfZjsxmhpjgVeraju3FzyfrnC/EHhDkh/diXUEuLiqztuJ10q7zBaNWnXWouvPL34gyR7AkVX1aeB1dGeWOgD4C/oWS5KTgXurm+f/M8C/6pefRjchGnQTe/1ckif0jx2S5MnD/UnSY7kHrzHbN8n1i+5/vKoWhkoenOQG4CHgnG1etyfw/iQH0u2Fv62q7kvyRuA9/eu+y6NT714AXJrkJuBz9OfqrKqbk/xH4JP9l8bDwKuAb0z575SW5DBJNcdhjGqFLRpJGin34CVppNyDl6SRssBL0khZ4CVppCzwkjRSFnhJGqn/D0a1r5YN95COAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-5.5279e-01, -5.4505e-02,  5.0748e-01],\n",
       "                      [-3.4533e-01,  3.0731e-01, -5.1183e-01],\n",
       "                      [ 2.3189e-01,  5.7039e-01, -3.4992e-01],\n",
       "                      [-3.3794e-01,  4.3167e-01, -2.2515e-01],\n",
       "                      [-4.8794e-01, -1.4271e-02, -3.2353e-02],\n",
       "                      [-3.8118e-01,  5.5686e-01, -3.1543e-01],\n",
       "                      [-2.7674e-01,  3.3989e-01, -5.4604e-01],\n",
       "                      [-4.1002e-02, -5.0681e-02, -1.1691e-01],\n",
       "                      [ 7.3957e-02,  2.9289e-01, -4.7357e-01],\n",
       "                      [-2.8643e-01,  3.8994e-01,  8.7458e-02],\n",
       "                      [-1.9412e-01, -4.3171e-01,  4.3643e-02],\n",
       "                      [-2.9545e-01,  1.2403e-01,  2.6011e-01],\n",
       "                      [ 4.0251e-01,  3.2213e-01,  3.4501e-01],\n",
       "                      [-6.4638e-02, -4.7906e-01,  3.0121e-01],\n",
       "                      [ 5.0586e-01,  3.6985e-02,  5.0435e-01],\n",
       "                      [ 1.4829e-01, -7.0631e-02, -4.0702e-01],\n",
       "                      [ 1.6555e-01,  2.0795e-01, -4.9713e-01],\n",
       "                      [-4.0583e-02, -1.7739e-01,  4.0382e-01],\n",
       "                      [ 5.2145e-01,  5.7799e-01,  2.6976e-01],\n",
       "                      [-3.0505e-01, -9.0395e-03,  1.8461e-01],\n",
       "                      [-2.1861e-01,  3.9419e-01,  9.7574e-02],\n",
       "                      [-3.7045e-01,  1.5073e-01, -3.8640e-01],\n",
       "                      [ 4.6424e-01,  2.6078e-01,  1.3372e-01],\n",
       "                      [-2.9874e-01,  5.0427e-01, -3.5299e-01],\n",
       "                      [ 4.0544e-01, -3.9139e-01, -5.5101e-01],\n",
       "                      [-2.6228e-01,  3.6038e-02, -4.1892e-01],\n",
       "                      [ 3.6823e-01, -5.4511e-01, -2.4665e-01],\n",
       "                      [-3.8312e-01,  2.5587e-01,  3.1296e-03],\n",
       "                      [ 2.0928e-02, -6.9719e-03,  1.3184e-01],\n",
       "                      [-1.0659e-01,  8.6012e-02,  3.5078e-01],\n",
       "                      [-3.8615e-01, -2.3147e-01,  5.5442e-01],\n",
       "                      [-4.0579e-01, -6.5536e-02,  4.6812e-01],\n",
       "                      [-2.1663e-01,  2.0557e-01, -4.2036e-01],\n",
       "                      [ 2.4949e-01, -1.8152e-01, -2.3767e-01],\n",
       "                      [-3.7460e-01, -3.9047e-02, -3.6124e-01],\n",
       "                      [ 4.3852e-02, -4.2680e-04, -4.2550e-01],\n",
       "                      [ 5.7875e-01,  1.0776e-01, -3.3652e-02],\n",
       "                      [ 5.0702e-01,  1.6862e-01, -5.9020e-03],\n",
       "                      [-1.9085e-01, -9.9873e-02,  4.6894e-01],\n",
       "                      [ 5.0383e-01,  2.5560e-01, -1.4083e-01],\n",
       "                      [-1.7262e-01,  3.3619e-01,  4.8044e-01],\n",
       "                      [-2.6634e-01,  5.6648e-01, -6.6452e-02],\n",
       "                      [-3.0805e-01,  4.1745e-01, -1.2844e-01],\n",
       "                      [-4.9929e-01, -2.6225e-02,  4.8895e-01],\n",
       "                      [-2.2178e-01, -4.1599e-01,  6.3300e-02],\n",
       "                      [ 6.0192e-03,  3.1320e-01, -2.2926e-01],\n",
       "                      [ 3.5584e-01, -2.6372e-01,  5.6005e-01],\n",
       "                      [ 1.3952e-01,  1.2529e-02,  4.2126e-01],\n",
       "                      [ 2.4737e-01,  3.7335e-01, -1.8840e-01],\n",
       "                      [-2.3386e-01, -2.9435e-01,  4.1158e-01],\n",
       "                      [-1.1663e-02, -4.5259e-01,  1.4817e-01],\n",
       "                      [ 1.5173e-01,  4.6733e-01, -3.7326e-01],\n",
       "                      [ 7.0488e-02,  1.4076e-01,  2.6894e-03],\n",
       "                      [-2.4651e-01, -4.7111e-01,  4.4321e-01],\n",
       "                      [ 1.5354e-01,  5.6648e-01, -4.7456e-01],\n",
       "                      [-1.3697e-01, -1.6979e-02,  2.0867e-02],\n",
       "                      [ 2.5352e-01,  4.1982e-01,  2.4378e-01],\n",
       "                      [ 2.8885e-01,  1.2951e-01, -3.3397e-01],\n",
       "                      [-1.5579e-01, -3.7208e-01, -3.2709e-01],\n",
       "                      [ 1.5893e-01,  2.7166e-03,  2.1378e-01],\n",
       "                      [-2.4784e-02,  3.4784e-01, -6.7352e-02],\n",
       "                      [ 1.9609e-02,  3.9078e-01,  3.0404e-01],\n",
       "                      [ 4.4832e-01,  4.1059e-01, -2.7175e-01],\n",
       "                      [ 2.6235e-02, -1.6948e-03,  1.7936e-01],\n",
       "                      [ 4.7404e-01,  2.6388e-01,  2.1473e-01],\n",
       "                      [ 6.2800e-03, -9.4895e-02, -4.4544e-01],\n",
       "                      [-5.7439e-01, -1.8710e-01,  3.9411e-02],\n",
       "                      [-4.2899e-01, -2.0012e-01,  3.8250e-01],\n",
       "                      [ 4.0811e-01,  5.2974e-01, -5.9557e-02],\n",
       "                      [ 4.7537e-01, -4.4696e-02, -5.2343e-01],\n",
       "                      [-2.5410e-01,  3.4546e-01, -5.6337e-01],\n",
       "                      [ 4.6925e-01,  2.1865e-01,  4.6458e-01],\n",
       "                      [-5.0679e-01,  5.0622e-01, -3.1745e-01],\n",
       "                      [ 3.8244e-01, -4.4178e-01,  3.7308e-01],\n",
       "                      [-4.1348e-01, -2.8505e-01, -5.7374e-01],\n",
       "                      [ 5.4783e-01,  3.4544e-01, -4.4073e-01],\n",
       "                      [-3.7729e-01,  1.2942e-01,  1.6486e-02],\n",
       "                      [ 1.3533e-01,  5.3043e-02, -5.0361e-01],\n",
       "                      [ 3.2327e-01, -4.3433e-01,  4.4002e-01],\n",
       "                      [-2.9132e-01,  4.3906e-01,  1.5293e-01],\n",
       "                      [-1.0768e-01,  3.6784e-02, -4.0856e-01],\n",
       "                      [-1.7157e-01,  5.0304e-01, -3.7880e-01],\n",
       "                      [ 4.0804e-01,  4.6315e-01, -3.2150e-01],\n",
       "                      [ 5.0551e-01,  1.3826e-02,  1.9283e-01],\n",
       "                      [ 5.5192e-01, -2.8117e-01,  4.1231e-01],\n",
       "                      [-5.5959e-01,  3.4459e-01,  2.5233e-01],\n",
       "                      [-2.7499e-01,  2.1862e-01, -2.2437e-01],\n",
       "                      [ 1.4228e-01, -8.8595e-02,  2.8219e-01],\n",
       "                      [ 2.0701e-01,  1.1741e-01, -1.5199e-02],\n",
       "                      [ 2.2685e-01,  6.2749e-02, -3.9696e-02],\n",
       "                      [ 5.1529e-01, -4.7766e-01, -2.6097e-01],\n",
       "                      [-5.7524e-01,  1.8170e-01,  1.6409e-01],\n",
       "                      [ 2.8464e-01, -4.9647e-01, -4.2652e-01],\n",
       "                      [-6.3360e-02,  8.1410e-02,  2.2993e-01],\n",
       "                      [ 1.6619e-01, -3.5509e-01,  3.4585e-01],\n",
       "                      [-4.4000e-01, -8.3687e-02, -5.0419e-01],\n",
       "                      [ 5.6044e-01, -4.8730e-02, -1.3761e-01],\n",
       "                      [ 1.6776e-01, -8.2291e-02,  5.2101e-01],\n",
       "                      [-4.6744e-02,  3.2550e-01,  4.4317e-01],\n",
       "                      [ 2.6529e-01, -2.4714e-01,  2.2061e-01],\n",
       "                      [-1.4332e-01, -5.5898e-01, -2.9092e-01],\n",
       "                      [ 2.2077e-01,  8.8051e-02, -2.6070e-01],\n",
       "                      [-4.8258e-01,  1.4590e-01,  1.7871e-01],\n",
       "                      [-2.8600e-01,  3.7800e-01,  2.4957e-01],\n",
       "                      [-4.4645e-01,  2.1917e-01, -5.7706e-01],\n",
       "                      [-2.2759e-01,  2.1569e-01,  2.9771e-01],\n",
       "                      [-1.5645e-01,  9.5703e-02, -2.6995e-01],\n",
       "                      [ 1.8960e-01,  3.3414e-01,  3.5483e-01],\n",
       "                      [-1.8590e-01, -7.4804e-02, -5.3323e-01],\n",
       "                      [-1.0283e-01, -7.8741e-02,  3.4773e-01],\n",
       "                      [ 5.1159e-01, -4.1770e-01, -2.9309e-01],\n",
       "                      [ 2.1878e-01, -1.3977e-02, -2.5061e-01],\n",
       "                      [-7.3850e-02,  9.2278e-02,  4.8069e-01],\n",
       "                      [ 1.0455e-01, -2.2123e-01, -5.4296e-01],\n",
       "                      [ 4.3743e-01,  1.5556e-01,  4.9217e-01],\n",
       "                      [-2.2834e-01,  3.4416e-01,  5.5661e-01],\n",
       "                      [ 2.4479e-01,  4.5799e-01,  2.2065e-01],\n",
       "                      [-7.8734e-02,  1.7505e-01, -2.1385e-01],\n",
       "                      [ 5.1620e-01,  1.2081e-02,  4.8197e-01],\n",
       "                      [ 2.7933e-01, -4.5097e-01, -6.3149e-02],\n",
       "                      [-8.3009e-03, -3.7374e-01, -1.6543e-01],\n",
       "                      [-3.6184e-01, -7.1993e-03, -1.6248e-01],\n",
       "                      [-3.6397e-01,  1.7363e-01,  2.4501e-01],\n",
       "                      [-3.9412e-01,  4.4118e-01,  5.3277e-01],\n",
       "                      [ 5.5427e-01,  2.1551e-01,  1.4000e-01],\n",
       "                      [-5.4606e-01, -9.7900e-02,  4.0166e-01],\n",
       "                      [ 2.5820e-01,  4.7633e-01,  4.3121e-01],\n",
       "                      [-5.2745e-01,  5.0406e-01, -4.1663e-01]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.0290,  0.0262,  0.3176, -0.5147,  0.3893,  0.1332,  0.5464,  0.4552,\n",
       "                      -0.0058, -0.1140,  0.5183,  0.4140,  0.2882,  0.1720, -0.1931,  0.5347,\n",
       "                       0.1906, -0.1894, -0.3084, -0.1605, -0.3644,  0.0571, -0.2871,  0.0600,\n",
       "                       0.4831,  0.2964,  0.1676, -0.3786, -0.2746, -0.4196, -0.5647,  0.0852,\n",
       "                       0.5423,  0.2352,  0.1611,  0.5367,  0.4194, -0.3355, -0.4077, -0.0834,\n",
       "                       0.0364, -0.3894,  0.0440, -0.2821, -0.1568,  0.2191, -0.0572, -0.0644,\n",
       "                       0.3942,  0.3124,  0.4258,  0.5518, -0.4998,  0.2857,  0.4615,  0.5740,\n",
       "                      -0.2557, -0.1775,  0.5211, -0.0076,  0.3371, -0.2337, -0.5510, -0.0141,\n",
       "                       0.1356, -0.1887,  0.0116, -0.4437, -0.3953, -0.4678,  0.2424,  0.3694,\n",
       "                       0.4283, -0.4249, -0.2494, -0.3175,  0.1587,  0.3873,  0.0204, -0.3957,\n",
       "                       0.1778, -0.3315, -0.0896,  0.0465,  0.2142,  0.5053,  0.5434, -0.0621,\n",
       "                       0.3419, -0.5734,  0.3874, -0.1529, -0.2193,  0.1881,  0.5360,  0.1951,\n",
       "                      -0.4643,  0.1118, -0.3275,  0.0732, -0.0113,  0.2135, -0.3161,  0.0200,\n",
       "                      -0.1698,  0.3377, -0.5759,  0.4887, -0.3340, -0.0853, -0.2558, -0.2884,\n",
       "                      -0.0167, -0.1691,  0.5038,  0.2895,  0.1439, -0.4194, -0.4516, -0.3965,\n",
       "                       0.4511,  0.4847,  0.1598, -0.1380,  0.3080,  0.0648,  0.1194,  0.1111])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 0.0857,  0.0714, -0.0636,  ..., -0.0837,  0.0657,  0.0555],\n",
       "                      [-0.0381,  0.0482,  0.0836,  ..., -0.0837, -0.0466, -0.0740],\n",
       "                      [-0.0337,  0.0569,  0.0036,  ..., -0.0511, -0.0370, -0.0108],\n",
       "                      ...,\n",
       "                      [ 0.0761, -0.0658,  0.0218,  ...,  0.0189,  0.0035,  0.0197],\n",
       "                      [-0.0111, -0.0617, -0.0408,  ..., -0.0772, -0.0137, -0.0823],\n",
       "                      [-0.0114, -0.0144,  0.0014,  ...,  0.0650, -0.0838,  0.0169]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.0167,  0.0689, -0.0494,  0.0509, -0.0569, -0.0519,  0.0704, -0.0345,\n",
       "                       0.0064,  0.0333, -0.0011,  0.0688,  0.0690,  0.0464,  0.0600, -0.0329,\n",
       "                      -0.0824,  0.0583,  0.0110,  0.0309, -0.0778, -0.0069, -0.0613, -0.0285,\n",
       "                       0.0703, -0.0039, -0.0291, -0.0621, -0.0273,  0.0677,  0.0874,  0.0418,\n",
       "                      -0.0046,  0.0502, -0.0443,  0.0010,  0.0477,  0.0488, -0.0162,  0.0614,\n",
       "                       0.0389,  0.0400, -0.0760,  0.0019,  0.0838,  0.0520,  0.0486,  0.0473,\n",
       "                      -0.0623, -0.0074,  0.0186, -0.0885, -0.0551,  0.0104, -0.0103,  0.0089,\n",
       "                      -0.0800, -0.0356, -0.0762, -0.0358, -0.0180,  0.0549, -0.0254,  0.0171,\n",
       "                       0.0794,  0.0268, -0.0126, -0.0743, -0.0370, -0.0200,  0.0673, -0.0223,\n",
       "                       0.0787,  0.0598, -0.0241, -0.0042,  0.0384,  0.0132, -0.0478, -0.0231,\n",
       "                       0.0341, -0.0058, -0.0114,  0.0381, -0.0483, -0.0625, -0.0599,  0.0782,\n",
       "                       0.0599, -0.0537, -0.0165, -0.0267,  0.0448, -0.0470,  0.0423, -0.0639,\n",
       "                       0.0358,  0.0541,  0.0304,  0.0761,  0.0511, -0.0071,  0.0562, -0.0843,\n",
       "                      -0.0805,  0.0427,  0.0641, -0.0051, -0.0594, -0.0654, -0.0588,  0.0879,\n",
       "                      -0.0449, -0.0676, -0.0629,  0.0742, -0.0224, -0.0775, -0.0783,  0.0756,\n",
       "                      -0.0045, -0.0241, -0.0823,  0.0604,  0.0404,  0.0525, -0.0518, -0.0251])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[ 0.0669,  0.0110, -0.0846, -0.0080, -0.0202,  0.0527,  0.0289,  0.0140,\n",
       "                        0.0848, -0.0347,  0.0448, -0.0252,  0.0529,  0.0222,  0.0226, -0.0183,\n",
       "                        0.0060,  0.0456,  0.0775,  0.0448, -0.0665,  0.0179,  0.0140, -0.0608,\n",
       "                        0.0674, -0.0441,  0.0635, -0.0794,  0.0829,  0.0360,  0.0195,  0.0699,\n",
       "                       -0.0121,  0.0632,  0.0200,  0.0283, -0.0753, -0.0719, -0.0467, -0.0575,\n",
       "                        0.0850, -0.0146, -0.0633,  0.0579,  0.0419,  0.0492,  0.0010, -0.0447,\n",
       "                       -0.0473,  0.0547, -0.0414,  0.0431,  0.0237, -0.0266, -0.0154, -0.0784,\n",
       "                       -0.0512, -0.0796,  0.0313, -0.0854,  0.0753,  0.0369,  0.0329,  0.0396,\n",
       "                       -0.0489,  0.0660, -0.0193,  0.0133,  0.0859,  0.0744, -0.0086,  0.0844,\n",
       "                        0.0125,  0.0814,  0.0384, -0.0115, -0.0161,  0.0091,  0.0291,  0.0146,\n",
       "                       -0.0702, -0.0756, -0.0072,  0.0276,  0.0732, -0.0366, -0.0422, -0.0438,\n",
       "                        0.0742,  0.0817, -0.0236, -0.0655, -0.0756,  0.0098,  0.0245,  0.0048,\n",
       "                       -0.0715, -0.0460,  0.0861, -0.0595, -0.0293, -0.0751,  0.0805, -0.0046,\n",
       "                        0.0591, -0.0008, -0.0340, -0.0645, -0.0458,  0.0748, -0.0177,  0.0082,\n",
       "                       -0.0210, -0.0478, -0.0607,  0.0844, -0.0700,  0.0595,  0.0090,  0.0801,\n",
       "                       -0.0861, -0.0529,  0.0658,  0.0006,  0.0365,  0.0593, -0.0484, -0.0657],\n",
       "                      [-0.0764,  0.0786,  0.0383,  0.0666, -0.0294,  0.0193, -0.0384, -0.0115,\n",
       "                        0.0321,  0.0342,  0.0486,  0.0605, -0.0429, -0.0355,  0.0098,  0.0500,\n",
       "                        0.0213,  0.0878,  0.0033, -0.0031, -0.0503,  0.0339, -0.0172, -0.0469,\n",
       "                       -0.0058, -0.0143, -0.0592, -0.0742,  0.0167, -0.0467, -0.0804, -0.0785,\n",
       "                        0.0322, -0.0053,  0.0321,  0.0056, -0.0140, -0.0652,  0.0128,  0.0231,\n",
       "                        0.0789,  0.0701,  0.0098,  0.0138, -0.0411,  0.0408, -0.0852,  0.0337,\n",
       "                       -0.0569,  0.0695, -0.0530,  0.0245, -0.0207,  0.0459, -0.0274, -0.0388,\n",
       "                        0.0808, -0.0550, -0.0700, -0.0373, -0.0745,  0.0347, -0.0723, -0.0414,\n",
       "                       -0.0635,  0.0258,  0.0141, -0.0634, -0.0215, -0.0400,  0.0214, -0.0822,\n",
       "                        0.0522, -0.0584, -0.0042,  0.0785, -0.0355,  0.0829, -0.0542, -0.0475,\n",
       "                       -0.0210, -0.0066,  0.0306, -0.0063, -0.0403,  0.0689, -0.0679, -0.0167,\n",
       "                       -0.0723,  0.0227,  0.0005,  0.0425,  0.0046, -0.0019, -0.0222, -0.0923,\n",
       "                       -0.0301,  0.0278, -0.0033, -0.0542,  0.0444, -0.0226,  0.0432, -0.0639,\n",
       "                        0.0799, -0.0686,  0.0696, -0.0527, -0.0754,  0.0093,  0.0654, -0.0640,\n",
       "                       -0.0148, -0.0262, -0.0234, -0.0359,  0.0575, -0.0520, -0.0417,  0.0678,\n",
       "                       -0.0229,  0.0385, -0.0510,  0.0269, -0.0445, -0.0639,  0.0548, -0.0820],\n",
       "                      [ 0.0478, -0.0328,  0.0600, -0.0712, -0.0573, -0.0028,  0.0769,  0.0411,\n",
       "                       -0.0348, -0.0401, -0.0662,  0.0138, -0.0065, -0.0856, -0.0597, -0.0494,\n",
       "                        0.0847, -0.0142,  0.0805,  0.0709, -0.0630,  0.0060, -0.0037, -0.0825,\n",
       "                        0.0600,  0.0524,  0.0841, -0.0773, -0.0676,  0.0400, -0.0556, -0.0329,\n",
       "                       -0.0267, -0.0048,  0.0451,  0.0841,  0.0770,  0.0119,  0.0562,  0.0853,\n",
       "                        0.0168, -0.0282, -0.0716,  0.0401,  0.0309, -0.0822, -0.0436,  0.0281,\n",
       "                       -0.0251, -0.0118, -0.0768,  0.0787, -0.0243, -0.0241,  0.0352,  0.0252,\n",
       "                       -0.0858, -0.0708, -0.0235, -0.0788,  0.0384,  0.0476,  0.0492,  0.0413,\n",
       "                       -0.0626,  0.0138,  0.0388, -0.0184,  0.0805,  0.0031,  0.0627,  0.0876,\n",
       "                        0.0518,  0.0785,  0.0759,  0.0399, -0.0056, -0.0726, -0.0722, -0.0430,\n",
       "                        0.0517,  0.0536, -0.0607, -0.0473,  0.0173, -0.0450, -0.0491, -0.0110,\n",
       "                       -0.0283,  0.0005,  0.0306, -0.0655, -0.0487,  0.0800,  0.0213, -0.0191,\n",
       "                        0.0403,  0.0382,  0.0659, -0.0073, -0.0235, -0.0643, -0.0155, -0.0513,\n",
       "                       -0.0797, -0.0007,  0.0108, -0.0343, -0.0838,  0.0566,  0.0883,  0.0789,\n",
       "                        0.0876, -0.0488,  0.0325, -0.0859, -0.0464, -0.0279,  0.0280,  0.0387,\n",
       "                        0.0050,  0.0473,  0.0045,  0.0515,  0.0556, -0.0390,  0.0126, -0.0543]])),\n",
       "             ('layer3.bias', tensor([-0.0473,  0.0092,  0.0457]))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17.91, 1.0, 2.8) 0.9099999999999997 tensor([[2]])\n",
      "(18.6653, 2.0, 2.6) 0.7552999999999996 tensor([[2]])\n",
      "(19.292198999999997, 3.0, 2.6) 0.6268989999999999 tensor([[2]])\n",
      "(19.812525169999997, 4.0, 2.6) 0.5203261700000001 tensor([[2]])\n",
      "(20.244395891099998, 5.0, 2.9) 0.43187072110000013 tensor([[0]])\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action = select_action(torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "    observation, reward, terminated, truncated, _ = env.step(2)\n",
    "    print(observation, env.temp_change, action)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diploma_env",
   "language": "python",
   "name": "diploma_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
